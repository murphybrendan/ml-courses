{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6D1l3xhZq5+t/2lsojwpQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murphybrendan/ml-courses/blob/main/huggingface/deep-rl/unit4/reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "Bix61QbZ_I8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LWvVLK6d-XO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7040c299-da60-45af-e648-efcd4ffca0c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
            "  Cloning https://github.com/ntasfi/PyGame-Learning-Environment.git to /tmp/pip-req-build-cedo2lfb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ntasfi/PyGame-Learning-Environment.git /tmp/pip-req-build-cedo2lfb\n",
            "  Resolved https://github.com/ntasfi/PyGame-Learning-Environment.git to commit 3dbe79dc0c35559bb441b9359948aabf9bb3d331\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/simoninithomas/gym-games\n",
            "  Cloning https://github.com/simoninithomas/gym-games to /tmp/pip-req-build-oe5l4_8q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/simoninithomas/gym-games /tmp/pip-req-build-oe5l4_8q\n",
            "  Resolved https://github.com/simoninithomas/gym-games to commit f31695e4ba028400628dc054ee8a436f28193f0b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.2)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from ple==0.0.1) (9.4.0)\n",
            "Requirement already satisfied: gym>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4) (0.25.2)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4) (67.7.2)\n",
            "Requirement already satisfied: pygame>=1.9.6 in /usr/local/lib/python3.10/dist-packages (from gym-games==1.0.4) (2.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.13.0->gym-games==1.0.4) (0.0.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.6.2)\n",
            "Building wheels for collected packages: ple, gym-games\n",
            "  Building wheel for ple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ple: filename=ple-0.0.1-py3-none-any.whl size=50769 sha256=93e30bcb465a0d8c13d110a78ea3324a9838522027042e0a88a8b0d07252b4ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ob4szbe/wheels/f8/31/ca/a64a7ce73540465412d82813780d062db53b90e3f42a4ecb7f\n",
            "  Building wheel for gym-games (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-games: filename=gym_games-1.0.4-py3-none-any.whl size=17305 sha256=1bedcb9f66ac5d8544017d8f2c27821fea69ba07dc6c30beeee7a5d8ba2e7c76\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7ob4szbe/wheels/ca/bf/6b/7d631626202ebb033c908a688d1862ff4d948c34cf621d7dc9\n",
            "Successfully built ple gym-games\n",
            "Installing collected packages: ple, gym-games\n",
            "Successfully installed gym-games-1.0.4 ple-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git git+https://github.com/simoninithomas/gym-games huggingface_hub imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "import imageio"
      ],
      "metadata": {
        "id": "G78nHc6B_rjz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "bI0BUKwt-6Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca74d69-314c-48fb-9420-e112cdd469fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "\n",
        "@dataclass\n",
        "class TrainingArgs:\n",
        "    n_training_episodes: int\n",
        "    max_t: int\n",
        "    gamma: float\n",
        "    lr: float\n",
        "    env_id: str\n",
        "    hidden_layers: list[int] = field(default_factory=list)\n",
        "    reporting_interval: int = -1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-09hSn0Latr",
        "outputId": "f7aa88cb-6c99-4812-99f3-7ba41740f733"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Reinforce"
      ],
      "metadata": {
        "id": "3rGdmZ8hAVpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Network"
      ],
      "metadata": {
        "id": "mQrpPU5IAik7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforcePolicy(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_layers=[64, 128]):\n",
        "        layers = [state_dim] + hidden_layers\n",
        "        modules = []\n",
        "        for i in range(len(layers)):\n",
        "            modules.append(nn.Linear(layers[i], layers[i+1]))\n",
        "            modules.append(nn.ReLU())\n",
        "        modules.append(nn.Linear(layers[-1], action_dim))\n",
        "        self.layers = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layers(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "    def act(self, state: np.ndarray) -> tuple[int, float]:\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        action_probs = self.forward(state)\n",
        "        m = Categorical(action_probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz0cMiaTAhna",
        "outputId": "494e4a79-4f93-4628-98c0-0c64d3b42df8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Agent"
      ],
      "metadata": {
        "id": "3u931P3dCkvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import trange\n",
        "\n",
        "class ReinforceAgent:\n",
        "\n",
        "    def __init__(self, env: gym.Env, args: TrainingArgs):\n",
        "        self.args = args\n",
        "\n",
        "        self.env = gym.make(args.env_id)\n",
        "        self.policy = ReinforcePolicy(env.observation_space.shape[0], env.action_space.n, args.hidden_layers).to(device)\n",
        "        self.optim = torch.optim.Adam(self.policy.parameters(), lr=args.lr)\n",
        "\n",
        "    def generate_episode(self):\n",
        "        self._rewards = []\n",
        "        self._log_probs = []\n",
        "        state = self.env.reset()\n",
        "        for i in range(self.args.max_t):\n",
        "            action, log_prob = self.policy(state)\n",
        "            self._log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            self._rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        self._scores.append(sum(self._rewards))\n",
        "\n",
        "    def optimization_step(self):\n",
        "        # Calculated the discounted returns, iterating backwards\n",
        "        returns = deque()\n",
        "        for t in range(len(self._rewards))[::-1]:\n",
        "            g_t1 = returns[0] if returns else 0\n",
        "            returns.appendleft(self._rewards[t] + self.args.gamma*g_t1)\n",
        "\n",
        "        assert len(returns) == len(self._rewards)\n",
        "\n",
        "        # Normalize the returns\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        # Calculate the loss for each action/return\n",
        "        losses = []\n",
        "        for log_prob, discounted_return in (zip(self._log_probs, returns)):\n",
        "            losses.append(-log_prob * discounted_return)\n",
        "        loss = torch.cat(losses).sum()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    def train(self):\n",
        "        self._scores = []\n",
        "        progress_bar = trange(self.args.n_training_episodes)\n",
        "        for i in progress_bar:\n",
        "            self.generate_episode()\n",
        "            self.optimization_step()\n",
        "\n",
        "            if self.args.reporting_interval > 0 and i % self.args.reporting_interval == 0:\n",
        "                progress_bar.set_description(f\"Episode {i} Average Score: {np.mean(self._scores):.2f}\")"
      ],
      "metadata": {
        "id": "3MLDq_YYCmNT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run environments"
      ],
      "metadata": {
        "id": "haH4kJa6KQXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cartpole"
      ],
      "metadata": {
        "id": "uObPzPMFKTbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_args = TrainingArgs(\n",
        "    env_id=\"CartPole-v1\",\n",
        "    n_training_episodes=1000,\n",
        "    max_t=1000,\n",
        "    gamma=1.0,\n",
        "    lr=1e-2,\n",
        "    hidden_layers=[16, 32],\n",
        "    reporting_interval=100\n",
        ")\n",
        "\n",
        "agent = ReinforceAgent(cartpole_args)\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "bt5SrVQDKSWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PixelCopter"
      ],
      "metadata": {
        "id": "e9eHGZLTNP23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pixelcopter_args = {\n",
        "    env_id = \"Pixelcopter-PLE-v0\",\n",
        "    n_training_episodes = 50000,\n",
        "    max_t = 10000,\n",
        "    gamma = 0.99,\n",
        "    lr = 1e-4,\n",
        "    hidden_layers = [64, 128],\n",
        "    reporting_interval = 1000\n",
        "}"
      ],
      "metadata": {
        "id": "KLLGK_srNZ1d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}